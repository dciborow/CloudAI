{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model Schema File For Publishing\n",
    "\n",
    "This notebook will generate the schema file needed for the ML service that will be deployed. Further it will also utilize much of the Python code in the score.py file as a test to ensure it works so we aren't chasing ghosts after deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and constants\n",
    "\n",
    "These are the imports and constants required for the functionality that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model File ./modelfile/factory.pkl\n",
      "Schema File ./modelfile/factory.schema\n"
     ]
    }
   ],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()\n",
    "\n",
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on\n",
    "\n",
    "from azureml.datacollector import ModelDataCollector\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema\n",
    "\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess\n",
    "from azure.storage.blob import ContentSettings\n",
    "\n",
    "import pandas\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Azure storage and file name information\n",
    "AZURE_STORAGE_ACCOUNT_NAME = \"<STORAGE_ACCOUNT_NAME>\"\n",
    "AZURE_STORAGE_ACCOUNT_KEY = \"<STORAGE_ACCOUNT_KEY>\"\n",
    "AZURE_STORAGE_CONTAINER_NAME = \"readydemo\"\n",
    "AZURE_STORAGE_BLOB_NAME = \"factory.pkl\"\n",
    "AZURE_STORAGE_BLOB_NAME_SCHEMA = \"factory.schema\"\n",
    "\n",
    "LOCAL_SYSTEM_DIRECTORY = \"modelfile\"\n",
    "\n",
    "MODEL_FILE_LOCAL = \"./{}/{}\".format(LOCAL_SYSTEM_DIRECTORY,AZURE_STORAGE_BLOB_NAME)\n",
    "SCHEMA_FILE_LOCAL = \"./{}/{}\".format(LOCAL_SYSTEM_DIRECTORY,AZURE_STORAGE_BLOB_NAME_SCHEMA)\n",
    "\n",
    "print(\"Model File {}\".format(MODEL_FILE_LOCAL))\n",
    "print(\"Schema File {}\".format(SCHEMA_FILE_LOCAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download model from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local directory already exists!\n",
      "Local model file exists and was deleted\n",
      "Model file downloaded\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the directory to put the file exists\n",
    "if not os.path.exists(LOCAL_SYSTEM_DIRECTORY):\n",
    "    os.makedirs(LOCAL_SYSTEM_DIRECTORY)\n",
    "    print('DONE creating a local directory!')\n",
    "else:\n",
    "    print('Local directory already exists!')\n",
    "    \n",
    "# If the file exists, make sure to delete it.\n",
    "if os.path.isfile(MODEL_FILE_LOCAL):\n",
    "    os.remove(MODEL_FILE_LOCAL)\n",
    "    print(\"Local model file exists and was deleted\")\n",
    "    \n",
    "# Pull model back to the local system\n",
    "az_blob_service = BlockBlobService(account_name=AZURE_STORAGE_ACCOUNT_NAME, account_key=AZURE_STORAGE_ACCOUNT_KEY)\n",
    "az_blob_service.get_blob_to_path(AZURE_STORAGE_CONTAINER_NAME, AZURE_STORAGE_BLOB_NAME, MODEL_FILE_LOCAL)\n",
    "\n",
    "print(\"Model file downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Python Code\n",
    "This code is essentially a duplicate of the code found in the score.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init function used to initialize the model from the local file that we downloaded\n",
    "def init():\n",
    "    global inputs_dc, prediction_dc, localFilePath\n",
    "    import os\n",
    "    from sklearn.externals import joblib\n",
    "\n",
    "    # load the model file\n",
    "    global model\n",
    "    model = joblib.load(localFilePath)\n",
    "\n",
    "    inputs_dc = ModelDataCollector(localFilePath, identifier=\"inputs\")\n",
    "    prediction_dc = ModelDataCollector(localFilePath, identifier=\"prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model and get the prediction(s)\n",
    "def run(input_df):\n",
    "    import json\n",
    "    \n",
    "    inputs_dc.collect(input_df)\n",
    "\n",
    "    pred = model.predict(input_df)\n",
    "    prediction_dc.collect(pred)\n",
    "    return json.dumps(str(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the model schema\n",
    "Generates the schema and uploads it to the storage account in the same container as the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection is in debug mode. Set environment variable AML_MODEL_DC_STORAGE_ENABLED to 'true' to send data to the cloud (http://aka.ms/amlmodeldatacollection).\n",
      "Data collection is in debug mode. Set environment variable AML_MODEL_DC_STORAGE_ENABLED to 'true' to send data to the cloud (http://aka.ms/amlmodeldatacollection).\n",
      "Schema generated\n",
      "Schema uploaded\n"
     ]
    }
   ],
   "source": [
    "# Turn on data collection debug mode to view output in stdout\n",
    "os.environ[\"AML_MODEL_DC_DEBUG\"] = 'true'\n",
    "\n",
    "localFilePath = MODEL_FILE_LOCAL\n",
    "\n",
    "# Initialize the model\n",
    "init()\n",
    "\n",
    "# Create inputs for the model\n",
    "df = pandas.DataFrame(data=[[45.9842594460449, 150.513223075022, 277.294013981084, 1.0, 1.0]], columns=['temp', 'volt','rotate','time', 'id'])\n",
    "inputs = {\"input_df\": SampleDefinition(DataTypes.PANDAS, df)}\n",
    "  \n",
    "# Genereate the schema using the inputs\n",
    "generate_schema(run_func=run, inputs=inputs, filepath=SCHEMA_FILE_LOCAL)\n",
    "print(\"Schema generated\")\n",
    "\n",
    "# Upload the schema to blob storage\n",
    "az_blob_service.create_blob_from_path(\n",
    "        AZURE_STORAGE_CONTAINER_NAME,\n",
    "        AZURE_STORAGE_BLOB_NAME_SCHEMA,\n",
    "        SCHEMA_FILE_LOCAL,\n",
    "        content_settings=ContentSettings(content_type='application/json'))\n",
    "\n",
    "print(\"Schema uploaded\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dangreadytr1 ubuntuvm",
   "language": "python",
   "name": "dangreadytr1_ubuntuvm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
