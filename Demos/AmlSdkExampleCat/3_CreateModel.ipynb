{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Variables ##\n",
    "\n",
    "><b>Pre-requisites</b>\n",
    ">\n",
    "> Azure ML SDK has been installed.\n",
    ">\n",
    "> You have run <b>jupyter notebook</b> from a command line that was based in your project directory.\n",
    ">\n",
    "> You have successfully executed <b>az login</b> from said command line.\n",
    "\n",
    "This notebook retrieves the training data from a predetermined location in Azure and separates it inot training and test data sets.\n",
    "\n",
    "From there, it trains a model, tests it, reports statistics on it (and they aren't great, but this is just an exercise) and finally writes the model out to disk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Variables ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation/acquisition\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import ContentSettings\n",
    "\n",
    "\n",
    "# Using the ski-kit leanr, this will fail if it has not been added as a dependency\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Serialize models\n",
    "import pickle\n",
    "\n",
    "# Do NOT change this stoarge information as it is the training data provided for you.\n",
    "AZURE_STORAGE_ACCOUNT_NAME = \"catjulydemodata\"\n",
    "AZURE_STORAGE_ACCOUNT_KEY = \"94nA2VT9YR8cWuT0eqUMM8uWJiRgTUpkGwONKKD/WsZIPC8UDH6PBQFSgTTh01qbY9WqCF0HoWvOnb1wRmIM6Q==\"\n",
    "AZURE_STORAGE_DATA_CONTAINER_NAME = \"factorydemo\"\n",
    "AZURE_STORAGE_DATA_BLOB_NAME = \"factorydataset.csv\"\n",
    "\n",
    "#AZURE_STORAGE_MODEL_CONTAINER_NAME = \"factorymodel\"\n",
    "ML_MODEL_NAME = \"factorymodel.pkl\"\n",
    "\n",
    "LOCAL_SYSTEM_DATA_DIRECTORY = \"./TrainingData\"\n",
    "LOCAL_SYSTEM_DATA_FILE = \"{}/{}\".format(LOCAL_SYSTEM_DATA_DIRECTORY, AZURE_STORAGE_DATA_BLOB_NAME)\n",
    "LOCAL_SYSTEM_MODEL_FILE = \"./{}\".format(ML_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Training Data ##\n",
    "\n",
    "Downloads the pre-created training data for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the local directory for the training data (provided for you)\n",
    "if not os.path.exists(LOCAL_SYSTEM_DATA_DIRECTORY):\n",
    "    os.makedirs(LOCAL_SYSTEM_DATA_DIRECTORY)\n",
    "    print('DONE creating a local directory!')\n",
    "else:\n",
    "    print('Local directory already exists!')\n",
    "    \n",
    "#Create the blob service\n",
    "az_blob_service = BlockBlobService(account_name=AZURE_STORAGE_ACCOUNT_NAME, account_key=AZURE_STORAGE_ACCOUNT_KEY)\n",
    "\n",
    "#Download the training data\n",
    "az_blob_service.get_blob_to_path(AZURE_STORAGE_DATA_CONTAINER_NAME, AZURE_STORAGE_DATA_BLOB_NAME, LOCAL_SYSTEM_DATA_FILE)\n",
    "\n",
    "allTrainingDataFrame = pd.read_csv(LOCAL_SYSTEM_DATA_FILE)\n",
    "\n",
    "allTrainingDataFrame.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets for training and testing \n",
    "\n",
    "We first search the data to determine how many successes and how many failures we have. In that process we determine that\n",
    "the data is heavily weighted towards succesful devices. \n",
    "\n",
    "To ensure that we have reasonable data for both testing and training we will:\n",
    "    - Split the data up into two buckets, succesful data and failed data\n",
    "    - Randomly choose approximately 70% of the data from each bucket for training and the remaining 30% for testing.\n",
    "    \n",
    "This type of splity stratifies the data to ensure that neither set is too heavlily weighted to either class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the datasets into successful devices and failed devices.\n",
    "successrows = allTrainingDataFrame.loc[allTrainingDataFrame['state'] == 0]\n",
    "failurerows = allTrainingDataFrame.loc[allTrainingDataFrame['state'] == 1]\n",
    "\n",
    "print(\"Of the {} records, {} are devices that are OK, and {} are in a failure state\".format(len(allTrainingDataFrame), len(successrows), len(failurerows)))\n",
    "print(\"\")\n",
    "\n",
    "# Numpy will create us a boolean array of the length we want randomly selecting true and false. This allows us \n",
    "# to choose 0.7(~70%) of the succesful and failed devices.\n",
    "successmsk = np.random.rand(len(successrows)) < 0.7\n",
    "failuremsk = np.random.rand(len(failurerows)) < 0.7\n",
    "\n",
    "# For training take the 70% of items for each in the training set\n",
    "trainingDataFrame = pd.concat([successrows[successmsk], failurerows[failuremsk]])\n",
    "# For testing take the remaining 30% of items for each in the testing set\n",
    "testingDataFrame = pd.concat([successrows[~successmsk], failurerows[~failuremsk]])\n",
    "\n",
    "print(\"Training Data: {} records\".format(len(trainingDataFrame)))\n",
    "print(trainingDataFrame.head(5))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Testing Data : {} records\".format(len(testingDataFrame)))\n",
    "print(testingDataFrame.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "The next step after splitting the data is to train a model. We are using a two class decision model here which is well suited to\n",
    "DecisionTree. \n",
    "\n",
    "We use __ski-kit learn__ DecisionTreeClassifier as the model, train it with our training data then report on feature importance after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to know which of the columns in our dataset are to be used as features, so we identify them here.\n",
    "featureColumnNames = [\"temp\", \"volt\", \"rotate\", \"time\", \"id\"] \n",
    "\n",
    "# We set up two sets, one of features and one for labels\n",
    "trainingFeatures = trainingDataFrame[featureColumnNames]\n",
    "trainingLabels = trainingDataFrame[\"state\"]\n",
    "\n",
    "# fit/train the model with the training data\n",
    "decisionTreeClassifier = DecisionTreeClassifier()\n",
    "decisionTreeClassifier.fit(trainingFeatures, trainingLabels)\n",
    "\n",
    "# Report on the feature importance, note that the time and id fields hold very little value to \n",
    "# determining our success or failure rate. In a real case, these fields would likely be left out\n",
    "# of the decision making process before production.\n",
    "print(\"Results of fitting the classifier with the training data:\")\n",
    "print(\"Feature Columns: {}\".format(featureColumnNames))\n",
    "print(\"Feature Importance: {}\".format(decisionTreeClassifier.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, time and id have very little to do with the model test, but we want them in so when results are returned we can correctly record the results for the device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model \n",
    "\n",
    "Using our testing data (the 30% of the original dataset) we score the model by predicting results only passing in the feature columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the testing data (30%) predict, get the features and send them in to predict\n",
    "testdata = testingDataFrame[featureColumnNames]\n",
    "results = decisionTreeClassifier.predict(testdata)\n",
    "\n",
    "# Turn the results to pandas dataframe and rename the one column to prediction for easier reading of results.\n",
    "pd_results = pd.DataFrame(results)\n",
    "pd_results.columns = [\"prediction\"]\n",
    "\n",
    "# Merge the test data set with the predictions, but because this is a subset of the main dataset the index of each\n",
    "# row will caust a pandas.concat to give us a jagged matrix. Create a new dataframe with the index reset so that the \n",
    "# concat works as expected.\n",
    "mergetest = testingDataFrame.reset_index(drop=True)\n",
    "resultset = pd.concat([mergetest, pd_results], axis=1)\n",
    "\n",
    "# Visualize a few things\n",
    "print(\"Prediction results using the testing data:\")\n",
    "print(resultset.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research the results of the model ##\n",
    "\n",
    "Using the test done above, we can determine how the model performed. To do so:\n",
    "\n",
    "    Use the model to calculate accuracy\n",
    "    Calculate the TP, FP, TN, FN\n",
    "    Calcualte precision, recall and fscore of the model\n",
    "\n",
    "We calculage precision and recall because accuracy is not enough. We should find that the model performs, without any tweaking, at close to 90% but precision and recall are closer to 70%. In a real world situation this likely is not sufficient and would need tweaking by the data scientist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do some connecting of data to results\n",
    "actualOkState = len(resultset.loc[resultset['state'] == 0.0])\n",
    "actualFailState = len(resultset.loc[resultset['state'] == 1.0])\n",
    "resultOk = len(resultset.loc[resultset['prediction'] == 0.0])\n",
    "resultFail = len(resultset.loc[resultset['prediction'] == 1.0])\n",
    "\n",
    "# Calculate True/False Negatives\n",
    "trueNegative = len(resultset.loc[(resultset['state'] == 0.0) & (resultset['prediction'] == 0.0)]) # TN\n",
    "falseNegative = len(resultset.loc[(resultset['state'] == 1.0) & (resultset['prediction'] == 0.0)]) #FN\n",
    "\n",
    "# Calculate True/False Positives\n",
    "truePositive = len(resultset.loc[(resultset['state'] == 1.0) & (resultset['prediction'] == 1.0)]) #TP\n",
    "falsePositive = len(resultset.loc[(resultset['state'] == 0.0) & (resultset['prediction'] == 1.0)]) #FP\n",
    "\n",
    "# Precision is percentage of failed prediction that are correct. Where Precision = TP/(TP+FP)\n",
    "precision = truePositive / (truePositive+falsePositive)\n",
    "# Recall is the percentage of failures that correctly identified. Where  Recall = TP/(TP+FN)\n",
    "recall = truePositive / (truePositive+falseNegative)\n",
    "# F score : f = 2*(precision*recall)/(precision + recall)\n",
    "fscore = 2* ((precision*recall)/(precision+recall))\n",
    "\n",
    "#Print out the score of the model and information about the result set\n",
    "print(\"Model accuracy {}\".format(decisionTreeClassifier.score(testdata,testingDataFrame[\"state\"])))\n",
    "print(\"\")\n",
    "print(\"Result set size: {0}\".format(len(resultset)))\n",
    "print(\"\")\n",
    "print(\"Info on devices that are OK:\")\n",
    "print(\"Known good = {} , predicted good {}\".format(actualOkState, resultOk))\n",
    "print(\"\")\n",
    "print(\"Info on devices that have failed:\")\n",
    "print(\"Known failed = {} , predicted failed {}\".format(actualFailState, resultFail))\n",
    "print(\"\")\n",
    "print(\"Precision {}\".format(precision))\n",
    "print(\"Recall {}\".format(recall))\n",
    "print(\"FScore {}\".format(fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so these results are pretty abysmal in the eyes of a data scientist, but we will continue on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "This section will perform the following steps:\n",
    "    \n",
    "- Use pickle to serialize the model to a local file\n",
    "- Upload the model file to an Azure Storage Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the local file and dump the model to it.    \n",
    "filestream = open(LOCAL_SYSTEM_MODEL_FILE, 'wb')\n",
    "pickle.dump(decisionTreeClassifier, filestream)\n",
    "filestream.close()\n",
    "print(\"Model file was serialized to local path {}\".format(LOCAL_SYSTEM_MODEL_FILE))\n",
    "\n",
    "# Upload the local file to Azure Storage\n",
    "#az_blob_service.create_blob_from_path(\n",
    "#    AZURE_STORAGE_MODEL_CONTAINER_NAME,\n",
    "#    AZURE_STORAGE_MODEL_BLOB_NAME,\n",
    "#    LOCAL_SYSTEM_MODEL_FILE,\n",
    "#    content_settings=ContentSettings(content_type='application/octet-stream'))\n",
    "\n",
    "#print(\"Model {} was uploaded to storage in {} container\".format(AZURE_STORAGE_MODEL_BLOB_NAME, AZURE_STORAGE_MODEL_CONTAINER_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
